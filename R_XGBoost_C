###########################################################################
#################### IMPORTA LIBS E DEFINE ################################
###########################################################################
gc()
rm(list = ls())
library(caret)
library(car)
library(C50)
library(ISLR)
library(xgboost)
library(MatrixModels)
library(Matrix)
library(gmodels)
library(e1071)
library(devtools)
library(parallel)
library(doParallel)
library(Information)
library(InformationValue)
library(ROCR)
library(dplyr)
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)

###########################################################################
#################### IMPORTO BASE E TRATO VARIAVEIS #######################
###########################################################################
AF_RAIZ                 <- read.csv(file = "C:\\Users\\snf6123\\OneDrive for Business\\Compartilhado com Todos\\PERL\\SAS\\MACHINE LEARNING\\REDES NEURAIS\\AF\\BASE_AF_10X.csv", 
                         sep = ";", 
                         stringsAsFactors = FALSE
                         #,na.strings = c("",".","NA")
                         )
#AF_RAIZ[is.na(AF_RAIZ)] <- 0
AF_RAIZ$STATUS_N        <- as.numeric(AF_RAIZ$STATUS_N)
x <- substr(AF_RAIZ$SAFRA_EVENTO_MES,1,2)
y <- substr(AF_RAIZ$SAFRA_EVENTO_MES,3,5)
z <- substr(AF_RAIZ$SAFRA_EVENTO_MES,6,9)
AF_RAIZ$SAFRA_EVENTO_MES<- lubridate::ymd(paste0(year  = z, 
                                                 month = y, 
                                                 day   = x))
AF_RAIZ <- subset(AF_RAIZ, MOSAIC_CODE = c(6,7,8)) 

###########################################################################
#################### SELECIONA AMOSTRAGEM #################################
###########################################################################
str(AF_RAIZ)
set.seed(666)
AF_RAIZ_random  <- AF_RAIZ[order(runif(NROW(AF_RAIZ))), ]
AF_treino_valid <- subset(AF_RAIZ_random, SAFRA_EVENTO_MES != '2018-12-01') 
smp_siz         <- floor(0.75*nrow(AF_treino_valid))
for_train       <- sample(seq_len(nrow(AF_treino_valid)), size = smp_siz)

AF_treino     <- AF_treino_valid[for_train, ]
AF_validacao  <- AF_treino_valid[-for_train, ]

teste         <- subset(AF_RAIZ_random,SAFRA_EVENTO_MES=='2018-12-01')
teste_1       <- subset(teste, STATUS_N == 1)
teste_0       <- subset(teste, STATUS_N == 0)
smp_siz       <- floor(0.19*nrow(teste_0))
for_train     <- sample(seq_len(nrow(teste_0)), size = smp_siz)
AF_teste_0    <- teste_0[for_train,]
AF_teste      <- rbind(teste_1,AF_teste_0)
AF_teste      <- AF_teste[,]

#CHECO SE A PROPORCAO DAS BASES ESTÃO CORRETA:
prop.table(table(AF_RAIZ$STATUS_N))
prop.table(table(AF_treino$STATUS_N))
prop.table(table(AF_validacao$STATUS_N))
prop.table(table(AF_teste$STATUS_N))

###########################################################################
#################### AJUSTO OS CAMPOS #####################################
###########################################################################
for (i in 1:NCOL(AF_treino))
  AF_treino[,i]     <- as.numeric(AF_treino[,i])
for (i in 1:NCOL(AF_validacao))
  AF_validacao[,i]  <- as.numeric(AF_validacao[,i])
for (i in 1:NCOL(AF_teste))
  AF_teste[,i]      <- as.numeric(AF_teste[,i])
AF_treino$SAFRA_EVENTO_MES    <- as.Date(AF_treino$SAFRA_EVENTO_MES, 
                                         origin = "1970-01-01")
AF_validacao$SAFRA_EVENTO_MES <- as.Date(AF_validacao$SAFRA_EVENTO_MES, 
                                         origin = "1970-01-01")
AF_teste$SAFRA_EVENTO_MES     <- as.Date(AF_teste$SAFRA_EVENTO_MES, 
                                         origin = "1970-01-01")

###########################################################################
#################### CONFIGURO PRIMEIRA XGBOOST ###########################
###########################################################################
AF_xgb <- xgboost(data            = data.matrix(AF_treino[,c(-1,-2,-3)]),
                  label           = AF_treino$STATUS_N,
                  verbose         = 0,    #define avisos como ligado
                  booster         = 'gblinear',
                  eta             = 0.003,     #melhor é 0.003: prevenção contra overfitting - VALORES TÍPICOS: [0.01, 0.2]
                  objective       = 'binary:logistic',
                  eval_metric     = 'error',  #erro correto para class.bin.
                  nrounds         = 300,
                  seed            = 666)
importance <- xgb.importance(model = AF_xgb)
importance$ajustado <- abs(importance$Weight)
importance$'%' <- importance$ajustado/sum(importance$ajustado)
importance$'% acum' <- importance[1,4]
for(i in 2:nrow(importance))
{
  importance[i,5] <- importance[i,4] + importance [i-1,5]
}
x11()
xgb.plot.importance(importance,
                    rel_to_first = FALSE,
                    main = "Importancia das Variaveis - 100%",
                    xlab = "Importancia na Resposta")
###########################################################################
#################### CONFIGURO SEGUNDA XGBOOST ############################
###########################################################################
to.remove <- subset(importance, `% acum` > 0.95)
AF_treino <- AF_treino[, !colnames(AF_treino) %in% to.remove$Feature]
AF_validacao <- AF_validacao[, !colnames(AF_validacao) %in% to.remove$Feature]
AF_teste <- AF_teste[, !colnames(AF_teste) %in% to.remove$Feature]
AF_treino$SCORE <- NULL

AF_xgb <- xgboost(data            = data.matrix(AF_treino[,c(-1,-2,-3)]),
                  label           = AF_treino$STATUS_N,
                  verbose         = 0,    #define avisos como ligado
                  booster         = 'gblinear',
                  eta             = 0.1,     #melhor é 0.003: prevenção contra overfitting - VALORES TÍPICOS: [0.01, 0.2]
                  objective       = 'binary:logistic',
                  eval_metric     = 'error',  #erro correto para class.bin.
                  nrounds         = 300,
                  seed            = 666)
importance <- xgb.importance(model = AF_xgb)
importance$ajustado <- abs(importance$Weight)
importance$'%' <- importance$ajustado/sum(importance$ajustado)
importance$'% acum' <- importance[1,4]
for(i in 2:nrow(importance))
{
  importance[i,5] <- importance[i,4] + importance [i-1,5]
}
x11()
xgb.plot.importance(importance,
                    rel_to_first = FALSE,
                    main = "Importancia das Variaveis - 95%",
                    xlab = "Importancia na Resposta")
###########################################################################
#################### PREDIÇÃO DA XGBOOST ##################################
###########################################################################
AF_treino$SCORE <- NULL
AF_treino$SCORE <- predict(AF_xgb,
                           data.matrix(AF_treino[,c(-1,-2,-3)]),
                           type = "prob")

AF_validacao$SCORE <- NULL
AF_validacao$SCORE <- predict(AF_xgb,
                              data.matrix(AF_validacao[,c(-1,-2,-3)]),
                              type = "prob")

AF_teste$SCORE <- NULL
AF_teste$SCORE <- predict(AF_xgb,
                          data.matrix(AF_teste[,c(-1,-2,-3)]),
                          type = "prob")
###########################################################################
#################### AVALIA A PERFORMANCE DO MODELO #######################
###########################################################################
pred_xgb_treino <- prediction(AF_treino$SCORE,
                              AF_treino$STATUS_N)
ROC_treino <-ROCR::performance(pred_xgb_treino,
                               measure = "tpr",
                               x.measure = "fpr")
ks_treino   <- max(attr(ROC_treino,
                       "y.values")[[1]] - (attr(ROC_treino,
                                                "x.values")[[1]]))

pred_xgb_valid <- prediction(AF_validacao$SCORE,
                             AF_validacao$STATUS_N)
ROC_valid <-ROCR::performance(pred_xgb_valid,
                        measure = "tpr",
                        x.measure = "fpr")
ks_valid   <- max(attr(ROC_valid,
                 "y.values")[[1]] - (attr(ROC_valid,
                                          "x.values")[[1]]))

pred_xgb_teste <- prediction(AF_teste$SCORE,
                             AF_teste$STATUS_N)
ROC_teste <-ROCR::performance(pred_xgb_teste,
                              measure = "tpr",
                              x.measure = "fpr")
ks_teste <- max(attr(ROC_teste,
                "y.values")[[1]] - (attr(ROC_teste,
                                         "x.values")[[1]]))

ks_treino
ks_valid
ks_teste

###########################################################################
#################### PLOTA GRÁFICOS #######################################
###########################################################################
x11()
plot(ROC_teste,
     col = "blue",
     main=paste0('KS Treino = ',
                 round(ks_treino*100,1),
                 '% - KS Valid. = ',
                 round(ks_valid*100,1),
                 '% - KS Teste = ',
                 round(ks_teste*100,1),'%'))
plot(ROC_treino,
     add = TRUE,
     col = "red")
plot(ROC_valid,
     add = TRUE,
     col = "yellow")
lines(x=c(0,1),
      y=c(0,1))

###########################################################################
#################### CALCULA AUC - AREA UNDER the CURVE ###################
###########################################################################
ROC.auc <- performance(pred_xgb_teste,measure = "auc")
unlist(ROC.auc@y.values)
###########################################################################
#################### CALCULA ESTABILIDADE DO KS ###########################
###########################################################################
ks_vector <- NULL
interaction_vector <- NULL
AF_treino$SCORE <- NULL
for(i in 1:200)
{
  base.teste.ks <- sample_n(AF_treino, 0.05*nrow(AF_treino), replace =TRUE)
  AF_xgb <- xgboost(data            = data.matrix(base.teste.ks[,c(-1,-2,-3)]),
                    label           = base.teste.ks$STATUS_N,
                    verbose         = 0,    #define avisos como off
                    booster         = 'gblinear',
                    eta             = 0.1,     #melhor é 0.003: prevenção contra overfitting - VALORES TÍPICOS: [0.01, 0.2]
                    objective       = 'binary:logistic',
                    eval_metric     = 'error',  #erro correto para class.bin.
                    nrounds         = 300,
                    seed            = 666)
  pred_treino_teste.ks <- predict(AF_xgb,
                                  data.matrix(base.teste.ks[,c(-1,-2,-3)]),
                                  type = "prob")
  pred_xgb_treino_teste.ks <- prediction(pred_treino_teste.ks,
                                         base.teste.ks$STATUS_N)
  ROC_treino_teste.ks <-ROCR::performance(pred_xgb_treino_teste.ks,
                                          measure = "tpr",
                                          x.measure = "fpr")
  ks_treino_teste.ks   <- max(attr(ROC_treino_teste.ks,
                                   "y.values")[[1]] - (attr(ROC_treino_teste.ks,
                                                            "x.values")[[1]]))
  ks_vector          <- rbind(ks_vector,
                              ks_treino_teste.ks)
  interaction_vector <- rbind(interaction_vector,
                              i)
  print(i)
}
ks.base <- data.frame('ks'=ks_vector, 'interaction'=interaction_vector)
ks.base
x11()
plot(ks.base$ks,
     type = "l",
     main = paste0("Distribuição KS x 200 amostras de Treino"),
     xlab = "Número da Interação",
     ylab = "KS",
     ylim = c(0,1),
     xlim = c(min(ks.base$interaction), max(ks.base$interaction)))

ks_vector <- NULL
interaction_vector <- NULL
AF_validacao$SCORE <- NULL
for(i in 1:200)
{
  base.teste.ks <- sample_n(AF_validacao, 0.05*nrow(AF_validacao), replace =TRUE)
  AF_xgb <- xgboost(data            = data.matrix(base.teste.ks[,c(-1,-2,-3)]),
                    label           = base.teste.ks$STATUS_N,
                    verbose         = 0,    #define avisos como off
                    booster         = 'gblinear',
                    eta             = 0.1,     #melhor é 0.003: prevenção contra overfitting - VALORES TÍPICOS: [0.01, 0.2]
                    objective       = 'binary:logistic',
                    eval_metric     = 'error',  #erro correto para class.bin.
                    nrounds         = 300,
                    seed            = 666)
  pred_valid_teste.ks <- predict(AF_xgb,
                                  data.matrix(base.teste.ks[,c(-1,-2,-3)]),
                                  type = "prob")
  pred_xgb_valid_teste.ks <- prediction(pred_valid_teste.ks,
                                         base.teste.ks$STATUS_N)
  ROC_valid_teste.ks <-ROCR::performance(pred_xgb_valid_teste.ks,
                                          measure = "tpr",
                                          x.measure = "fpr")
  ks_valid_teste.ks   <- max(attr(ROC_valid_teste.ks,
                                   "y.values")[[1]] - (attr(ROC_valid_teste.ks,
                                                            "x.values")[[1]]))
  ks_vector          <- rbind(ks_vector,
                              ks_valid_teste.ks)
  interaction_vector <- rbind(interaction_vector,
                              i)
  print(i)
}
ks.base <- data.frame('ks'=ks_vector, 'interaction'=interaction_vector)
ks.base
x11()
plot(ks.base$ks,
     type = "l",
     main = paste0("Distribuição KS x 200 amostras de Validacao"),
     xlab = "Número da Interação",
     ylab = "KS",
     ylim = c(0,1),
     xlim = c(min(ks.base$interaction), max(ks.base$interaction)))
###########################################################################
#################### PREPARA ALAVANCAGEM ##################################
###########################################################################
Alavancagem_Treino_C <- AF_treino %>% 
                        mutate(FX_SCORE = ifelse(AF_treino$SCORE <= quantile(AF_treino$SCORE, prob = seq(0,1,length=11),type = 5)[2],"J",
                                          ifelse(AF_treino$SCORE <= quantile(AF_treino$SCORE, prob = seq(0,1,length=11),type = 5)[3],"I",
                                          ifelse(AF_treino$SCORE <= quantile(AF_treino$SCORE, prob = seq(0,1,length=11),type = 5)[4],"H",
                                          ifelse(AF_treino$SCORE <= quantile(AF_treino$SCORE, prob = seq(0,1,length=11),type = 5)[5],"G",
                                          ifelse(AF_treino$SCORE <= quantile(AF_treino$SCORE, prob = seq(0,1,length=11),type = 5)[6],"F",
                                          ifelse(AF_treino$SCORE <= quantile(AF_treino$SCORE, prob = seq(0,1,length=11),type = 5)[7],"E",
                                          ifelse(AF_treino$SCORE <= quantile(AF_treino$SCORE, prob = seq(0,1,length=11),type = 5)[8],"D",
                                          ifelse(AF_treino$SCORE <= quantile(AF_treino$SCORE, prob = seq(0,1,length=11),type = 5)[9],"C",
                                          ifelse(AF_treino$SCORE <= quantile(AF_treino$SCORE, prob = seq(0,1,length=11),type = 5)[10],"B","A")))))))))) %>% 
                        group_by(FX_SCORE) %>% 
                        summarise(qtd_tot = n(),
                                  mix = n()/nrow(AF_treino),
                                  qtd_bom = sum(STATUS_N == 1, na.rm = T),
                                  qtd_mau = sum(STATUS_N == 0, na.rm = T)) %>% 
                        mutate(ODD = (qtd_bom/qtd_tot)/(qtd_mau/qtd_tot))
Alavancagem_Valid_C <- AF_validacao %>% 
  mutate(FX_SCORE = ifelse(AF_validacao$SCORE <= quantile(AF_validacao$SCORE, prob = seq(0,1,length=11),type = 5)[2],"J",
                    ifelse(AF_validacao$SCORE <= quantile(AF_validacao$SCORE, prob = seq(0,1,length=11),type = 5)[3],"I",
                                  ifelse(AF_validacao$SCORE <= quantile(AF_validacao$SCORE, prob = seq(0,1,length=11),type = 5)[4],"H",
                                         ifelse(AF_validacao$SCORE <= quantile(AF_validacao$SCORE, prob = seq(0,1,length=11),type = 5)[5],"G",
                                                ifelse(AF_validacao$SCORE <= quantile(AF_validacao$SCORE, prob = seq(0,1,length=11),type = 5)[6],"F",
                                                       ifelse(AF_validacao$SCORE <= quantile(AF_validacao$SCORE, prob = seq(0,1,length=11),type = 5)[7],"E",
                                                              ifelse(AF_validacao$SCORE <= quantile(AF_validacao$SCORE, prob = seq(0,1,length=11),type = 5)[8],"D",
                                                                     ifelse(AF_validacao$SCORE <= quantile(AF_validacao$SCORE, prob = seq(0,1,length=11),type = 5)[9],"C",
                                                                            ifelse(AF_validacao$SCORE <= quantile(AF_validacao$SCORE, prob = seq(0,1,length=11),type = 5)[10],"B","A")))))))))) %>% 
  group_by(FX_SCORE) %>% 
  summarise(qtd_tot = n(),
            mix = n()/nrow(AF_validacao),
            qtd_bom = sum(STATUS_N == 1, na.rm = T),
            qtd_mau = sum(STATUS_N == 0, na.rm = T)) %>% 
  mutate(ODD = (qtd_bom/qtd_tot)/(qtd_mau/qtd_tot))
Alavancagem_Teste_C <- AF_teste %>% 
  mutate(FX_SCORE = ifelse(AF_teste$SCORE <= quantile(AF_teste$SCORE, prob = seq(0,1,length=11),type = 5)[2],"J",
                           ifelse(AF_teste$SCORE <= quantile(AF_teste$SCORE, prob = seq(0,1,length=11),type = 5)[3],"I",
                                  ifelse(AF_teste$SCORE <= quantile(AF_teste$SCORE, prob = seq(0,1,length=11),type = 5)[4],"H",
                                         ifelse(AF_teste$SCORE <= quantile(AF_teste$SCORE, prob = seq(0,1,length=11),type = 5)[5],"G",
                                                ifelse(AF_teste$SCORE <= quantile(AF_teste$SCORE, prob = seq(0,1,length=11),type = 5)[6],"F",
                                                       ifelse(AF_teste$SCORE <= quantile(AF_teste$SCORE, prob = seq(0,1,length=11),type = 5)[7],"E",
                                                              ifelse(AF_teste$SCORE <= quantile(AF_teste$SCORE, prob = seq(0,1,length=11),type = 5)[8],"D",
                                                                     ifelse(AF_teste$SCORE <= quantile(AF_teste$SCORE, prob = seq(0,1,length=11),type = 5)[9],"C",
                                                                            ifelse(AF_teste$SCORE <= quantile(AF_teste$SCORE, prob = seq(0,1,length=11),type = 5)[10],"B","A")))))))))) %>% 
  group_by(FX_SCORE) %>% 
  summarise(qtd_tot = n(),
            mix = n()/nrow(AF_teste),
            qtd_bom = sum(STATUS_N == 1, na.rm = T),
            qtd_mau = sum(STATUS_N == 0, na.rm = T)) %>% 
  mutate(ODD = (qtd_bom/qtd_tot)/(qtd_mau/qtd_tot))

P_mau_acum_treino <- NULL
P_bom_acum_treino <- NULL
P_mau_acum_valid <- NULL
P_bom_acum_valid <- NULL
P_mau_acum_teste <- NULL
P_bom_acum_teste <- NULL
KS_treino <- NULL
KS_valid <- NULL
KS_teste <- NULL
for(i in 1:10)
{
  P_mau_acum_treino[i] <- sum(Alavancagem_Treino_C[1:i,5])/sum(Alavancagem_Treino_C[1:10,5])
  P_mau_acum_valid[i] <- sum(Alavancagem_Treino_C[1:i,5])/sum(Alavancagem_Treino_C[1:10,5])
  P_mau_acum_teste[i] <- sum(Alavancagem_Treino_C[1:i,5])/sum(Alavancagem_Treino_C[1:10,5])
  
  P_bom_acum_treino[i] <- sum(Alavancagem_Treino_C[1:i,4])/sum(Alavancagem_Treino_C[1:10,4])
  P_bom_acum_valid[i] <- sum(Alavancagem_Treino_C[1:i,4])/sum(Alavancagem_Treino_C[1:10,4])
  P_bom_acum_teste[i] <- sum(Alavancagem_Treino_C[1:i,4])/sum(Alavancagem_Treino_C[1:10,4])
  
  KS_treino[i] <- -P_mau_acum_treino[i] + P_bom_acum_treino[i]
  KS_valid[i] <- -P_mau_acum_valid[i] + P_bom_acum_valid[i] 
  KS_teste[i] <- -P_mau_acum_teste[i] + P_bom_acum_teste[i] 
}
Alavancagem_Treino_C$'%mau acum' <- P_mau_acum_treino
Alavancagem_Treino_C$'%bom acum' <- P_bom_acum_treino
Alavancagem_Treino_C$'KS' <- KS_treino

Alavancagem_Valid_C$'%mau acum' <- P_mau_acum_valid
Alavancagem_Valid_C$'%bom acum' <- P_bom_acum_valid
Alavancagem_Valid_C$'KS' <- KS_valid

Alavancagem_Teste_C$'%mau acum' <- P_mau_acum_teste
Alavancagem_Teste_C$'%bom acum' <- P_bom_acum_teste
Alavancagem_Teste_C$'KS' <- KS_teste
write.csv(Alavancagem_Treino_C,
          file = "C:\\Users\\snf6123\\OneDrive for Business\\Compartilhado com Todos\\PERL\\SAS\\MACHINE LEARNING\\XGBOOST\\AF\\2.PERFORMANCE\\ALAVANCAGEM_TREINO_C.csv")
write.csv(Alavancagem_Valid_C,
          file = "C:\\Users\\snf6123\\OneDrive for Business\\Compartilhado com Todos\\PERL\\SAS\\MACHINE LEARNING\\XGBOOST\\AF\\2.PERFORMANCE\\ALAVANCAGEM_VALIDACAO_C.csv")
write.csv(Alavancagem_Teste_C,
          file = "C:\\Users\\snf6123\\OneDrive for Business\\Compartilhado com Todos\\PERL\\SAS\\MACHINE LEARNING\\XGBOOST\\AF\\2.PERFORMANCE\\ALAVANCAGEM_TESTE_C.csv")
